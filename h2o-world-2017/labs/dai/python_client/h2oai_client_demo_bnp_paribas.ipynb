{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driverless AI: Using the Python API\n",
    "This notebook provides an H2OAI Client workflow, of model building and scoring, that parallels the Driverless AI workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow Steps\n",
    "\n",
    "**Build an Experiment with Python API:**\n",
    "1. Sign in\n",
    "2. Import train & test set/new data\n",
    "3. Specify columns to drop & select target column\n",
    "4. Set problem type, configuration, and accuracy settings\n",
    "5. Launch Experiement\n",
    "6. View Results\n",
    "    \n",
    "**Build an Experiment in Web UI and Access Through Python:**\n",
    "1. Get pointer to experiment\n",
    "    \n",
    "**Scoring:**\n",
    "1. Score using Driverless\n",
    "2. Score using the Scoring Package\n",
    "\n",
    "**Train H2O Model:**\n",
    "1. Train H2O Model on raw data\n",
    "2. Train H2O Model on transformed data\n",
    "3. Compare model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an Experiment with Python API\n",
    "\n",
    "### Import Datasets\n",
    "\n",
    "#### 1. Sign In\n",
    "To upload a dataset to the Driverless AI Server, pass in your credentials through the Client class which creates an authentication token to send to the Driverless AI Server. In plain english: to sign into the Driverless AI webage (which then sends requests to the Driverless Server) instantiate the Client class with your Driverless AI address and login credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2oai_client\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import h2o\n",
    "import requests\n",
    "import math\n",
    "from h2oai_client import Client, ModelParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = 'http://ip_where_driverless_is_running:12345'\n",
    "username = 'username'\n",
    "password = 'password'\n",
    "h2oai = Client(address = address, username = username, password = password)\n",
    "# make sure to use the same user name and password when signing in through the GUI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Equivalent Steps in Driverless: Signing In\n",
    "![Equivalent Steps in Driverless: Signing In](h2oai_client_images/sign_in_home_page_0.png)\n",
    "![Equivalent Steps in Driverless: Signing In](h2oai_client_images/skip_sign_in_home_page_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Upload Datasets\n",
    "\n",
    "In our example, we will use the BNP Paribas data.  We will split our data into training and validation so we can compare the performance of our Driverless AI model to an external model later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "import pandas as pd\n",
    "data_path = '/data/Kaggle/BNPParibas/BNPParibas-train.csv'\n",
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data\n",
    "import numpy as np\n",
    "train_indices = np.random.rand(len(data)) < 0.8\n",
    "train = data[train_indices]\n",
    "test = data[~train_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Data\n",
    "train_path = '/data/Kaggle/BNPParibas/train-80.csv'\n",
    "test_path = '/data/Kaggle/BNPParibas/valid-20.csv'\n",
    "train.to_csv(path_or_buf=train_path, index=False)\n",
    "test.to_csv(path_or_buf=test_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Dataset to Driverless \n",
    "train = h2oai.create_dataset_sync(train_path)\n",
    "test = h2oai.create_dataset_sync(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equivalent Steps in Driverless: Uploading Train & Test CSV Files\n",
    "![Equivalent Steps in Driverless: Uploading Train & Test CSV Files](h2oai_client_images/import_datasets_bnp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3. Set Columns: Target, Ignore, Weight, and Fold\n",
    "\n",
    "* Target Column: the column we are trying to predict\n",
    "* Ignore Columns: the columns we do not want to use as predictors such as ID columns, columns with data leakage, etc\n",
    "* Weight Column: the column that indicates the per row observation weights - if none each row will have an observation weight of 1 \n",
    "* Fold Column: the column that indicates the fold - if none the folds will be determined by Driverless AI\n",
    "\n",
    "For this example, we will be predicting **`target`** and we will ignore the column: **`ID`**.  We do not have a `weight_col` or `fold_col`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameters you want to pass to the UI\n",
    "target = \"target\"\n",
    "drop_cols = ['ID']\n",
    "weight_col = None\n",
    "fold_col = None\n",
    "time_col = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equivalent Steps in Driverless: Set Target & Ignored Columns\n",
    "![Equivalent Steps in Driverless: Set Target & Ignored Columns](h2oai_client_images/add_drop_test_bnp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Set Problem Type, Configuration & Accuracy Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-set parameters to pass model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_classification = True\n",
    "enable_gpus = True\n",
    "seed=True\n",
    "scorer_str = 'auc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-set accuracy knobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_value = 7\n",
    "time_value = 5\n",
    "interpretability = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Launch Experiment: Feature Engineering + Final Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = h2oai.start_experiment_sync(ModelParameters(\n",
    "    dataset_key=train.key,\n",
    "    target_col=target,\n",
    "    is_classification=is_classification,\n",
    "    cols_to_drop=drop_cols,\n",
    "    testset_key=test.key, \n",
    "    enable_gpus=enable_gpus,\n",
    "    seed=seed,\n",
    "    accuracy=accuracy_value, \n",
    "    time= time_value,\n",
    "    interpretability=interpretability, \n",
    "    scorer=scorer_str,\n",
    "    weight_col = weight_col,\n",
    "    fold_col = fold_col,\n",
    "    time_col = time_col\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Equivalent Steps in Driverless: Set the Knobs, Configuration & Launch\n",
    "![Equivalent Steps in Driverless: Set the Knobs](h2oai_client_images/set_experiment_settings_bnp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Equivalent Steps in Driverless: Launch Your Experiement](h2oai_client_images/exp_running_bnp.png)\n",
    "![Equivalent Steps in Driverless: Launch Your Experiement](h2oai_client_images/launch_experiment_bnp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. View Results\n",
    "\n",
    "You can use the experiment object to get the final model score on the training and testing data. This final model may be an ensemble model depending on the accuracy setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Score on Train Data: 0.7818\n",
      "Final Model Score on Test Data: 0.7822\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Model Score on Train Data: \" + str(round(experiment.train_score, 4)))\n",
    "print(\"Final Model Score on Test Data: \" + str(round(experiment.test_score, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equivalent Steps in Driverless: View Results\n",
    "![Equivalent Steps in Driverless: View Results](h2oai_client_images/experiment_done_bnp.png)\n",
    "\n",
    "#### All These Yellow Download Buttons Can Be Done through the Python API:\n",
    "* the munged test csv\n",
    "* the munged train csv\n",
    "* the predictions on the (holdout) train csv\n",
    "* the predictions on the test csv\n",
    "\n",
    "We will show an example of downloading the test predictions below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./test_preds.csv'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2oai.download(src_path = experiment.test_predictions_path, dest_dir = \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.877975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.948201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.932511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.899017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.931647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target.1\n",
       "0  0.877975\n",
       "1  0.948201\n",
       "2  0.932511\n",
       "3  0.899017\n",
       "4  0.931647"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = pd.read_csv(\"./test_preds.csv\")\n",
    "test_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an Experiment in Web UI and Access Through Python\n",
    "\n",
    "It is also possible to use the Python API to examine an experiment that was started through the Web UI using the experiment key.\n",
    "\n",
    "![Experiments List](h2oai_client_images/launch_experiment_bnp.png)\n",
    "\n",
    "#### 1. Get pointer to experiment\n",
    "\n",
    "You can get a pointer to the experiment by referencing the experiment key in the web UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['482b68']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_list = list(map(lambda x: x.key, h2oai.list_models(offset = 0, limit = 100)))\n",
    "experiment_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = h2oai.get_model_job(experiment_list[0]).entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring\n",
    "\n",
    "There are two ways to score: through Driverless AI or by downloading the Scoring Package.  The Scoring Package contains a python module and is able to score independently of Driverless AI. \n",
    "\n",
    "#### 1. Score using Driverless AI\n",
    "\n",
    "We will start out by scoring through Driverless AI.  This is equivalent to the `SCORE ON ANOTHER DATASET` button in the Web UI.  In the example below, we will score on the test data and download the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.877975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.948201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.932511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.899017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.931647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target.1\n",
       "0  0.877975\n",
       "1  0.948201\n",
       "2  0.932511\n",
       "3  0.899017\n",
       "4  0.931647"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = h2oai.make_prediction_sync(experiment.key, test_path)\n",
    "pred_path = h2oai.download(prediction.predictions_csv_path, '.')\n",
    "pred_table = pd.read_csv(pred_path)\n",
    "pred_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 2. Score using the Scoring Package\n",
    "\n",
    "We will download the scoring package and import the python scoring module to again score on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./scorer.zip'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the scoring package\n",
    "h2oai.download(experiment.scoring_package_path, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  scorer.zip\n",
      "   creating: scoring-package/\n",
      "  inflating: scoring-package/example_client.py  \n",
      "  inflating: scoring-package/requirements.txt  \n",
      "  inflating: scoring-package/server.py  \n",
      "  inflating: scoring-package/datatable-0.2.2+master.366.noomp-cp36-cp36m-linux_x86_64.whl  \n",
      "  inflating: scoring-package/example.py  \n",
      "  inflating: scoring-package/run_tcp_server.sh  \n",
      "  inflating: scoring-package/run_tcp_client.sh  \n",
      "  inflating: scoring-package/README.txt  \n",
      "  inflating: scoring-package/h2oaicore-1.0.9-cp36-cp36m-linux_x86_64.whl  \n",
      "  inflating: scoring-package/scoring_482b68_20171202204342_b6f33-1.0.0-py3-none-any.whl  \n",
      "  inflating: scoring-package/run_http_server.sh  \n",
      "  inflating: scoring-package/features.txt  \n",
      " extracting: scoring-package/server_requirements.txt  \n",
      "  inflating: scoring-package/run_http_client.sh  \n",
      "  inflating: scoring-package/run_example.sh  \n",
      "  inflating: scoring-package/scoring.thrift  \n",
      " extracting: scoring-package/client_requirements.txt  \n",
      "  inflating: scoring-package/h2o4gpu-0.1.0+master.8b1bad2-py36-none-any.whl  \n",
      "README.txt\n",
      "client_requirements.txt\n",
      "datatable-0.2.2+master.366.noomp-cp36-cp36m-linux_x86_64.whl\n",
      "example.py\n",
      "example_client.py\n",
      "features.txt\n",
      "h2o4gpu-0.1.0+master.8b1bad2-py36-none-any.whl\n",
      "h2oaicore-1.0.9-cp36-cp36m-linux_x86_64.whl\n",
      "requirements.txt\n",
      "run_example.sh\n",
      "run_http_client.sh\n",
      "run_http_server.sh\n",
      "run_tcp_client.sh\n",
      "run_tcp_server.sh\n",
      "scoring.thrift\n",
      "scoring_482b68_20171202204342_b6f33-1.0.0-py3-none-any.whl\n",
      "server.py\n",
      "server_requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Unzip scoring package and install the scoring python library\n",
    "unzip scorer\n",
    "\n",
    "cd scoring-package\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./scoring-package/scoring_482b68_20171202204342_b6f33-1.0.0-py3-none-any.whl\n",
      "Installing collected packages: scoring-482b68-20171202204342-b6f33\n",
      "Successfully installed scoring-482b68-20171202204342-b6f33-1.0.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Import scoring module\n",
    "pip install scoring-package/scoring_482b68_20171202204342_b6f33-1.0.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import nan\n",
    "from scoring_482b68_20171202204342_b6f33 import Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Create a singleton Scorer instance.\n",
    "# For optimal performance, create a Scorer instance once, and call score() or score_batch() multiple times.\n",
    "#\n",
    "scorer = Scorer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import test data\n",
    "test_data = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[Sat Dec  2 22:32:00 2017] [make_holdout_preds_subprocess] Begin GB used by self   26585: 0.150147',)\n",
      "('loading subprocess input data (model, X) took 0.01145 secs',)\n",
      "('[Sat Dec  2 22:32:00 2017] [predict_subprocess_xgboost] Begin GB used by self   26599: 0.152592',)\n",
      "('loading subprocess input data (model, X) took 0.01638 secs',)\n",
      "('subprocess predict took 0.68071 secs',)\n",
      "('saving subprocess output data (preds) took 0.00304 secs',)\n",
      "('[Sat Dec  2 22:32:01 2017] [predict_subprocess_xgboost] End GB used by self   26599: 0.239006',)\n",
      "('subprocess predict took 0.90996 secs',)\n",
      "('saving subprocess output data (preds) took 0.00395 secs',)\n",
      "('[Sat Dec  2 22:32:01 2017] [make_holdout_preds_subprocess] End GB used by self   26585: 0.193364',)\n",
      "('[Sat Dec  2 22:32:01 2017] [make_holdout_preds_subprocess] Begin GB used by self   26585: 0.193384',)\n",
      "('loading subprocess input data (model, X) took 0.01348 secs',)\n",
      "('[Sat Dec  2 22:32:01 2017] [predict_subprocess_xgboost] Begin GB used by self   26651: 0.150516',)\n",
      "('loading subprocess input data (model, X) took 0.02900 secs',)\n",
      "('subprocess predict took 0.81408 secs',)\n",
      "('saving subprocess output data (preds) took 0.00293 secs',)\n",
      "('[Sat Dec  2 22:32:02 2017] [predict_subprocess_xgboost] End GB used by self   26651: 0.249905',)\n",
      "('subprocess predict took 1.07206 secs',)\n",
      "('saving subprocess output data (preds) took 0.00288 secs',)\n",
      "('[Sat Dec  2 22:32:02 2017] [make_holdout_preds_subprocess] End GB used by self   26585: 0.224924',)\n",
      "('[Sat Dec  2 22:32:02 2017] [make_holdout_preds_subprocess] Begin GB used by self   26585: 0.224928',)\n",
      "('loading subprocess input data (model, X) took 0.01139 secs',)\n",
      "('[Sat Dec  2 22:32:02 2017] [predict_subprocess_xgboost] Begin GB used by self   26703: 0.150479',)\n",
      "('loading subprocess input data (model, X) took 0.02168 secs',)\n",
      "('subprocess predict took 0.70564 secs',)\n",
      "('saving subprocess output data (preds) took 0.00362 secs',)\n",
      "('[Sat Dec  2 22:32:03 2017] [predict_subprocess_xgboost] End GB used by self   26703: 0.238977',)\n",
      "('subprocess predict took 0.94488 secs',)\n",
      "('saving subprocess output data (preds) took 0.00468 secs',)\n",
      "('[Sat Dec  2 22:32:03 2017] [make_holdout_preds_subprocess] End GB used by self   26585: 0.252269',)\n",
      "('[Sat Dec  2 22:32:03 2017] [make_holdout_preds_subprocess] Begin GB used by self   26585: 0.252269',)\n",
      "('loading subprocess input data (model, X) took 0.00614 secs',)\n",
      "('[Sat Dec  2 22:32:03 2017] [predict_subprocess_xgboost] Begin GB used by self   26755: 0.154681',)\n",
      "('loading subprocess input data (model, X) took 0.01132 secs',)\n",
      "('subprocess predict took 0.58868 secs',)\n",
      "('saving subprocess output data (preds) took 0.00298 secs',)\n",
      "('[Sat Dec  2 22:32:04 2017] [predict_subprocess_xgboost] End GB used by self   26755: 0.224317',)\n",
      "('subprocess predict took 0.81931 secs',)\n",
      "('saving subprocess output data (preds) took 0.00289 secs',)\n",
      "('[Sat Dec  2 22:32:04 2017] [make_holdout_preds_subprocess] End GB used by self   26585: 0.262808',)\n",
      "('[Sat Dec  2 22:32:04 2017] [make_holdout_preds_subprocess] Begin GB used by self   26585: 0.262808',)\n",
      "('loading subprocess input data (model, X) took 0.01191 secs',)\n",
      "('[Sat Dec  2 22:32:04 2017] [predict_subprocess_xgboost] Begin GB used by self   26807: 0.152543',)\n",
      "('loading subprocess input data (model, X) took 0.03213 secs',)\n",
      "('subprocess predict took 0.75142 secs',)\n",
      "('saving subprocess output data (preds) took 0.00225 secs',)\n",
      "('[Sat Dec  2 22:32:05 2017] [predict_subprocess_xgboost] End GB used by self   26807: 0.247861',)\n",
      "('subprocess predict took 1.00511 secs',)\n",
      "('saving subprocess output data (preds) took 0.00533 secs',)\n",
      "('[Sat Dec  2 22:32:05 2017] [make_holdout_preds_subprocess] End GB used by self   26585: 0.271221',)\n",
      "('[Sat Dec  2 22:32:05 2017] [make_holdout_preds_subprocess] Begin GB used by self   26585: 0.271221',)\n",
      "('loading subprocess input data (model, X) took 0.00552 secs',)\n",
      "('[Sat Dec  2 22:32:05 2017] [predict_subprocess_xgboost] Begin GB used by self   26859: 0.156787',)\n",
      "('loading subprocess input data (model, X) took 0.01603 secs',)\n",
      "('subprocess predict took 0.78705 secs',)\n",
      "('saving subprocess output data (preds) took 0.00283 secs',)\n",
      "('[Sat Dec  2 22:32:06 2017] [predict_subprocess_xgboost] End GB used by self   26859: 0.232681',)\n",
      "('subprocess predict took 1.02407 secs',)\n",
      "('saving subprocess output data (preds) took 0.00281 secs',)\n",
      "('[Sat Dec  2 22:32:06 2017] [make_holdout_preds_subprocess] End GB used by self   26585: 0.271258',)\n",
      "('[Sat Dec  2 22:32:06 2017] [make_holdout_preds_subprocess] Begin GB used by self   26585: 0.271258',)\n",
      "('loading subprocess input data (model, X) took 0.00905 secs',)\n",
      "('[Sat Dec  2 22:32:06 2017] [predict_subprocess_xgboost] Begin GB used by self   26911: 0.154665',)\n",
      "('loading subprocess input data (model, X) took 0.03015 secs',)\n",
      "('subprocess predict took 0.91842 secs',)\n",
      "('saving subprocess output data (preds) took 0.00282 secs',)\n",
      "('[Sat Dec  2 22:32:07 2017] [predict_subprocess_xgboost] End GB used by self   26911: 0.247771',)\n",
      "('subprocess predict took 1.17181 secs',)\n",
      "('saving subprocess output data (preds) took 0.00419 secs',)\n",
      "('[Sat Dec  2 22:32:07 2017] [make_holdout_preds_subprocess] End GB used by self   26585: 0.271274',)\n",
      "('[Sat Dec  2 22:32:07 2017] [make_holdout_preds_subprocess] Begin GB used by self   26585: 0.271274',)\n",
      "('loading subprocess input data (model, X) took 0.00632 secs',)\n",
      "('[Sat Dec  2 22:32:08 2017] [predict_subprocess_xgboost] Begin GB used by self   26963: 0.148406',)\n",
      "('loading subprocess input data (model, X) took 0.02085 secs',)\n",
      "('subprocess predict took 0.67893 secs',)\n",
      "('saving subprocess output data (preds) took 0.00237 secs',)\n",
      "('[Sat Dec  2 22:32:08 2017] [predict_subprocess_xgboost] End GB used by self   26963: 0.232641',)\n",
      "('subprocess predict took 0.90814 secs',)\n",
      "('saving subprocess output data (preds) took 0.00246 secs',)\n",
      "('[Sat Dec  2 22:32:08 2017] [make_holdout_preds_subprocess] End GB used by self   26585: 0.271331',)\n",
      "('[Sat Dec  2 22:32:08 2017] [make_holdout_preds_subprocess] Begin GB used by self   26585: 0.271331',)\n",
      "('loading subprocess input data (model, X) took 0.00522 secs',)\n",
      "('[Sat Dec  2 22:32:09 2017] [predict_subprocess_xgboost] Begin GB used by self   27015: 0.150479',)\n",
      "('loading subprocess input data (model, X) took 0.01602 secs',)\n",
      "('subprocess predict took 0.98716 secs',)\n",
      "('saving subprocess output data (preds) took 0.00249 secs',)\n",
      "('[Sat Dec  2 22:32:10 2017] [predict_subprocess_xgboost] End GB used by self   27015: 0.234705',)\n",
      "('subprocess predict took 1.21093 secs',)\n",
      "('saving subprocess output data (preds) took 0.00294 secs',)\n",
      "('[Sat Dec  2 22:32:10 2017] [make_holdout_preds_subprocess] End GB used by self   26585: 0.273453',)\n",
      "('[Sat Dec  2 22:32:10 2017] [make_holdout_preds_subprocess] Begin GB used by self   26585: 0.273453',)\n",
      "('loading subprocess input data (model, X) took 0.01416 secs',)\n",
      "('[Sat Dec  2 22:32:10 2017] [predict_subprocess_xgboost] Begin GB used by self   27067: 0.142098',)\n",
      "('loading subprocess input data (model, X) took 0.04339 secs',)\n",
      "('subprocess predict took 0.97689 secs',)\n",
      "('saving subprocess output data (preds) took 0.00318 secs',)\n",
      "('[Sat Dec  2 22:32:11 2017] [predict_subprocess_xgboost] End GB used by self   27067: 0.27095',)\n",
      "('subprocess predict took 1.25606 secs',)\n",
      "('saving subprocess output data (preds) took 0.00313 secs',)\n",
      "('[Sat Dec  2 22:32:11 2017] [make_holdout_preds_subprocess] End GB used by self   26585: 0.281879',)\n",
      "('[Sat Dec  2 22:32:11 2017] [make_holdout_preds_subprocess] Begin GB used by self   26585: 0.281879',)\n",
      "('loading subprocess input data (model, X) took 0.00569 secs',)\n",
      "('[Sat Dec  2 22:32:11 2017] [predict_subprocess_xgboost] Begin GB used by self   27122: 0.144187',)\n",
      "('loading subprocess input data (model, X) took 0.01826 secs',)\n",
      "('subprocess predict took 0.78844 secs',)\n",
      "('saving subprocess output data (preds) took 0.00317 secs',)\n",
      "('[Sat Dec  2 22:32:12 2017] [predict_subprocess_xgboost] End GB used by self   27122: 0.236769',)\n",
      "('subprocess predict took 1.02336 secs',)\n",
      "('saving subprocess output data (preds) took 0.00282 secs',)\n",
      "('[Sat Dec  2 22:32:12 2017] [make_holdout_preds_subprocess] End GB used by self   26585: 0.281895',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[Sat Dec  2 22:32:12 2017] [make_holdout_preds_subprocess] Begin GB used by self   26585: 0.281895',)\n",
      "('loading subprocess input data (model, X) took 0.00903 secs',)\n",
      "('[Sat Dec  2 22:32:12 2017] [predict_subprocess_xgboost] Begin GB used by self   27174: 0.148386',)\n",
      "('loading subprocess input data (model, X) took 0.02847 secs',)\n",
      "('subprocess predict took 0.86315 secs',)\n",
      "('saving subprocess output data (preds) took 0.00243 secs',)\n",
      "('[Sat Dec  2 22:32:13 2017] [predict_subprocess_xgboost] End GB used by self   27174: 0.247665',)\n",
      "('subprocess predict took 1.10955 secs',)\n",
      "('saving subprocess output data (preds) took 0.00388 secs',)\n",
      "('[Sat Dec  2 22:32:13 2017] [make_holdout_preds_subprocess] End GB used by self   26585: 0.281915',)\n",
      "('[Sat Dec  2 22:32:13 2017] [make_holdout_preds_subprocess] Begin GB used by self   26585: 0.281915',)\n",
      "('loading subprocess input data (model, X) took 0.00740 secs',)\n",
      "('[Sat Dec  2 22:32:14 2017] [predict_subprocess_xgboost] Begin GB used by self   27226: 0.144175',)\n",
      "('loading subprocess input data (model, X) took 0.02080 secs',)\n",
      "('subprocess predict took 0.72917 secs',)\n",
      "('saving subprocess output data (preds) took 0.00374 secs',)\n",
      "('[Sat Dec  2 22:32:14 2017] [predict_subprocess_xgboost] End GB used by self   27226: 0.232542',)\n",
      "('subprocess predict took 0.96463 secs',)\n",
      "('saving subprocess output data (preds) took 0.00249 secs',)\n",
      "('[Sat Dec  2 22:32:14 2017] [make_holdout_preds_subprocess] End GB used by self   26585: 0.284045',)\n",
      "('[Sat Dec  2 22:32:14 2017] [make_holdout_preds_subprocess] Begin GB used by self   26585: 0.284045',)\n",
      "('loading subprocess input data (model, X) took 0.00455 secs',)\n",
      "('[Sat Dec  2 22:32:15 2017] [predict_subprocess_xgboost] Begin GB used by self   27278: 0.14841',)\n",
      "('loading subprocess input data (model, X) took 0.01440 secs',)\n",
      "('subprocess predict took 0.77337 secs',)\n",
      "('saving subprocess output data (preds) took 0.00290 secs',)\n",
      "('[Sat Dec  2 22:32:15 2017] [predict_subprocess_xgboost] End GB used by self   27278: 0.232587',)\n",
      "('subprocess predict took 0.99283 secs',)\n",
      "('saving subprocess output data (preds) took 0.00237 secs',)\n",
      "('[Sat Dec  2 22:32:15 2017] [make_holdout_preds_subprocess] End GB used by self   26585: 0.28407',)\n",
      "('[Sat Dec  2 22:32:15 2017] [make_holdout_preds_subprocess] Begin GB used by self   26585: 0.28407',)\n",
      "('loading subprocess input data (model, X) took 0.01184 secs',)\n",
      "('[Sat Dec  2 22:32:16 2017] [predict_subprocess_xgboost] Begin GB used by self   27330: 0.148365',)\n",
      "('loading subprocess input data (model, X) took 0.03836 secs',)\n",
      "('subprocess predict took 0.92635 secs',)\n",
      "('saving subprocess output data (preds) took 0.00233 secs',)\n",
      "('[Sat Dec  2 22:32:17 2017] [predict_subprocess_xgboost] End GB used by self   27330: 0.262377',)\n",
      "('subprocess predict took 1.18178 secs',)\n",
      "('saving subprocess output data (preds) took 0.00564 secs',)\n",
      "('[Sat Dec  2 22:32:17 2017] [make_holdout_preds_subprocess] End GB used by self   26585: 0.284099',)\n",
      "('[Sat Dec  2 22:32:17 2017] [make_holdout_preds_subprocess] Begin GB used by self   26585: 0.284099',)\n",
      "('loading subprocess input data (model, X) took 0.00520 secs',)\n",
      "('[Sat Dec  2 22:32:17 2017] [predict_subprocess_xgboost] Begin GB used by self   27382: 0.144212',)\n",
      "('loading subprocess input data (model, X) took 0.01641 secs',)\n",
      "('subprocess predict took 0.77139 secs',)\n",
      "('saving subprocess output data (preds) took 0.00284 secs',)\n",
      "('[Sat Dec  2 22:32:18 2017] [predict_subprocess_xgboost] End GB used by self   27382: 0.234652',)\n",
      "('subprocess predict took 0.99733 secs',)\n",
      "('saving subprocess output data (preds) took 0.00247 secs',)\n",
      "('[Sat Dec  2 22:32:18 2017] [make_holdout_preds_subprocess] End GB used by self   26585: 0.284131',)\n",
      "('[Sat Dec  2 22:32:18 2017] [make_holdout_preds_subprocess] Begin GB used by self   26585: 0.284131',)\n",
      "('loading subprocess input data (model, X) took 0.00811 secs',)\n",
      "('[Sat Dec  2 22:32:18 2017] [predict_subprocess_xgboost] Begin GB used by self   27434: 0.1442',)\n",
      "('loading subprocess input data (model, X) took 0.02857 secs',)\n",
      "('subprocess predict took 0.82433 secs',)\n",
      "('saving subprocess output data (preds) took 0.00285 secs',)\n",
      "('[Sat Dec  2 22:32:19 2017] [predict_subprocess_xgboost] End GB used by self   27434: 0.245604',)\n",
      "('subprocess predict took 1.06975 secs',)\n",
      "('saving subprocess output data (preds) took 0.00282 secs',)\n",
      "('[Sat Dec  2 22:32:19 2017] [make_holdout_preds_subprocess] End GB used by self   26585: 0.284148',)\n",
      "('[Sat Dec  2 22:32:19 2017] [make_holdout_preds_subprocess] Begin GB used by self   26585: 0.284148',)\n",
      "('loading subprocess input data (model, X) took 0.00728 secs',)\n",
      "('[Sat Dec  2 22:32:19 2017] [predict_subprocess_xgboost] Begin GB used by self   27489: 0.148394',)\n",
      "('loading subprocess input data (model, X) took 0.02159 secs',)\n",
      "('subprocess predict took 0.76154 secs',)\n",
      "('saving subprocess output data (preds) took 0.00325 secs',)\n",
      "('[Sat Dec  2 22:32:20 2017] [predict_subprocess_xgboost] End GB used by self   27489: 0.236761',)\n",
      "('subprocess predict took 1.00260 secs',)\n",
      "('saving subprocess output data (preds) took 0.00248 secs',)\n",
      "('[Sat Dec  2 22:32:20 2017] [make_holdout_preds_subprocess] End GB used by self   26585: 0.286286',)\n",
      "('[Sat Dec  2 22:32:20 2017] [make_holdout_preds_subprocess] Begin GB used by self   26585: 0.286286',)\n",
      "('loading subprocess input data (model, X) took 0.00401 secs',)\n",
      "('[Sat Dec  2 22:32:20 2017] [predict_subprocess_xgboost] Begin GB used by self   27541: 0.146313',)\n",
      "('loading subprocess input data (model, X) took 0.01528 secs',)\n",
      "('subprocess predict took 0.71646 secs',)\n",
      "('saving subprocess output data (preds) took 0.00292 secs',)\n",
      "('[Sat Dec  2 22:32:21 2017] [predict_subprocess_xgboost] End GB used by self   27541: 0.23466',)\n",
      "('subprocess predict took 0.94101 secs',)\n",
      "('saving subprocess output data (preds) took 0.00527 secs',)\n",
      "('[Sat Dec  2 22:32:21 2017] [make_holdout_preds_subprocess] End GB used by self   26585: 0.286286',)\n",
      "('[Sat Dec  2 22:32:21 2017] [make_holdout_preds_subprocess] Begin GB used by self   26585: 0.286286',)\n",
      "('loading subprocess input data (model, X) took 0.01096 secs',)\n",
      "('[Sat Dec  2 22:32:21 2017] [predict_subprocess_xgboost] Begin GB used by self   27593: 0.144187',)\n",
      "('loading subprocess input data (model, X) took 0.03404 secs',)\n",
      "('subprocess predict took 0.77250 secs',)\n",
      "('saving subprocess output data (preds) took 0.00312 secs',)\n",
      "('[Sat Dec  2 22:32:22 2017] [predict_subprocess_xgboost] End GB used by self   27593: 0.25002',)\n",
      "('subprocess predict took 1.03081 secs',)\n",
      "('saving subprocess output data (preds) took 0.00338 secs',)\n",
      "('[Sat Dec  2 22:32:22 2017] [make_holdout_preds_subprocess] End GB used by self   26585: 0.286319',)\n",
      "('[Sat Dec  2 22:32:22 2017] [make_holdout_preds_subprocess] Begin GB used by self   26585: 0.286319',)\n",
      "('loading subprocess input data (model, X) took 0.00437 secs',)\n",
      "('[Sat Dec  2 22:32:22 2017] [predict_subprocess_xgboost] Begin GB used by self   27645: 0.1442',)\n",
      "('loading subprocess input data (model, X) took 0.01402 secs',)\n",
      "('subprocess predict took 0.56905 secs',)\n",
      "('saving subprocess output data (preds) took 0.00237 secs',)\n",
      "('[Sat Dec  2 22:32:23 2017] [predict_subprocess_xgboost] End GB used by self   27645: 0.230416',)\n",
      "('subprocess predict took 0.79627 secs',)\n",
      "('saving subprocess output data (preds) took 0.00231 secs',)\n",
      "('[Sat Dec  2 22:32:23 2017] [make_holdout_preds_subprocess] End GB used by self   26585: 0.286331',)\n",
      "('[Sat Dec  2 22:32:23 2017] [make_holdout_preds_subprocess] Begin GB used by self   26585: 0.286331',)\n",
      "('loading subprocess input data (model, X) took 0.00936 secs',)\n",
      "('[Sat Dec  2 22:32:23 2017] [predict_subprocess_xgboost] Begin GB used by self   27697: 0.146297',)\n",
      "('loading subprocess input data (model, X) took 0.02967 secs',)\n",
      "('subprocess predict took 0.95737 secs',)\n",
      "('saving subprocess output data (preds) took 0.00336 secs',)\n",
      "('[Sat Dec  2 22:32:24 2017] [predict_subprocess_xgboost] End GB used by self   27697: 0.247665',)\n",
      "('subprocess predict took 1.20045 secs',)\n",
      "('saving subprocess output data (preds) took 0.00414 secs',)\n",
      "('[Sat Dec  2 22:32:24 2017] [make_holdout_preds_subprocess] End GB used by self   26585: 0.286347',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[Sat Dec  2 22:32:24 2017] [make_holdout_preds_subprocess] Begin GB used by self   26585: 0.286347',)\n",
      "('loading subprocess input data (model, X) took 0.00679 secs',)\n",
      "('[Sat Dec  2 22:32:24 2017] [predict_subprocess_xgboost] Begin GB used by self   27749: 0.146276',)\n",
      "('loading subprocess input data (model, X) took 0.02092 secs',)\n",
      "('subprocess predict took 0.79961 secs',)\n",
      "('saving subprocess output data (preds) took 0.00267 secs',)\n",
      "('[Sat Dec  2 22:32:25 2017] [predict_subprocess_xgboost] End GB used by self   27749: 0.23253',)\n",
      "('subprocess predict took 1.02942 secs',)\n",
      "('saving subprocess output data (preds) took 0.00245 secs',)\n",
      "('[Sat Dec  2 22:32:25 2017] [make_holdout_preds_subprocess] End GB used by self   26585: 0.286368',)\n",
      "('[Sat Dec  2 22:32:25 2017] [make_holdout_preds_subprocess] Begin GB used by self   26585: 0.286368',)\n",
      "('loading subprocess input data (model, X) took 0.00387 secs',)\n",
      "('[Sat Dec  2 22:32:25 2017] [predict_subprocess_xgboost] Begin GB used by self   27801: 0.148386',)\n",
      "('loading subprocess input data (model, X) took 0.01442 secs',)\n",
      "('subprocess predict took 0.69986 secs',)\n",
      "('saving subprocess output data (preds) took 0.00405 secs',)\n",
      "('[Sat Dec  2 22:32:26 2017] [predict_subprocess_xgboost] End GB used by self   27801: 0.234545',)\n",
      "('subprocess predict took 0.91936 secs',)\n",
      "('saving subprocess output data (preds) took 0.00508 secs',)\n",
      "('[Sat Dec  2 22:32:26 2017] [make_holdout_preds_subprocess] End GB used by self   26585: 0.286384',)\n",
      "('[Sat Dec  2 22:32:26 2017] [make_holdout_preds_subprocess] Begin GB used by self   26585: 0.286384',)\n",
      "('loading subprocess input data (model, X) took 0.01142 secs',)\n",
      "('[Sat Dec  2 22:32:26 2017] [predict_subprocess_xgboost] Begin GB used by self   27853: 0.144183',)\n",
      "('loading subprocess input data (model, X) took 0.03717 secs',)\n",
      "('subprocess predict took 0.89348 secs',)\n",
      "('saving subprocess output data (preds) took 0.00245 secs',)\n",
      "('[Sat Dec  2 22:32:27 2017] [predict_subprocess_xgboost] End GB used by self   27853: 0.260043',)\n",
      "('subprocess predict took 1.15582 secs',)\n",
      "('saving subprocess output data (preds) took 0.00248 secs',)\n",
      "('[Sat Dec  2 22:32:27 2017] [make_holdout_preds_subprocess] End GB used by self   26585: 0.286421',)\n"
     ]
    }
   ],
   "source": [
    "check = scorer.score_batch(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[Sat Dec  2 22:09:47 2017] [make_holdout_preds_subprocess] Begin GB used by self   21186: 0.09984',)\n",
      "('loading subprocess input data (model, X) took 0.01517 secs',)\n",
      "('[Sat Dec  2 22:09:47 2017] [predict_subprocess_xgboost] Begin GB used by self   21196: 0.095998',)\n",
      "('loading subprocess input data (model, X) took 0.01635 secs',)\n",
      "('subprocess predict took 0.69448 secs',)\n",
      "('saving subprocess output data (preds) took 0.00254 secs',)\n",
      "('[Sat Dec  2 22:09:48 2017] [predict_subprocess_xgboost] End GB used by self   21196: 0.171848',)\n",
      "('subprocess predict took 0.89756 secs',)\n",
      "('saving subprocess output data (preds) took 0.00167 secs',)\n",
      "('[Sat Dec  2 22:09:48 2017] [make_holdout_preds_subprocess] End GB used by self   21186: 0.142971',)\n",
      "('[Sat Dec  2 22:09:48 2017] [make_holdout_preds_subprocess] Begin GB used by self   21186: 0.143065',)\n",
      "('loading subprocess input data (model, X) took 0.01642 secs',)\n",
      "('[Sat Dec  2 22:09:48 2017] [predict_subprocess_xgboost] Begin GB used by self   21228: 0.0876175',)\n",
      "('loading subprocess input data (model, X) took 0.02932 secs',)\n",
      "('subprocess predict took 0.82295 secs',)\n",
      "('saving subprocess output data (preds) took 0.00207 secs',)\n",
      "('[Sat Dec  2 22:09:49 2017] [predict_subprocess_xgboost] End GB used by self   21228: 0.180089',)\n",
      "('subprocess predict took 1.04369 secs',)\n",
      "('saving subprocess output data (preds) took 0.00132 secs',)\n",
      "('[Sat Dec  2 22:09:49 2017] [make_holdout_preds_subprocess] End GB used by self   21186: 0.173756',)\n",
      "('[Sat Dec  2 22:09:49 2017] [make_holdout_preds_subprocess] Begin GB used by self   21186: 0.173756',)\n",
      "('loading subprocess input data (model, X) took 0.00732 secs',)\n",
      "('[Sat Dec  2 22:09:49 2017] [predict_subprocess_xgboost] Begin GB used by self   21260: 0.0939008',)\n",
      "('loading subprocess input data (model, X) took 0.02165 secs',)\n",
      "('subprocess predict took 0.74276 secs',)\n",
      "('saving subprocess output data (preds) took 0.00164 secs',)\n",
      "('[Sat Dec  2 22:09:50 2017] [predict_subprocess_xgboost] End GB used by self   21260: 0.160633',)\n",
      "('subprocess predict took 0.94816 secs',)\n",
      "('saving subprocess output data (preds) took 0.00272 secs',)\n",
      "('[Sat Dec  2 22:09:50 2017] [make_holdout_preds_subprocess] End GB used by self   21186: 0.17383',)\n",
      "('[Sat Dec  2 22:09:50 2017] [make_holdout_preds_subprocess] Begin GB used by self   21186: 0.175927',)\n",
      "('loading subprocess input data (model, X) took 0.00486 secs',)\n",
      "('[Sat Dec  2 22:09:50 2017] [predict_subprocess_xgboost] Begin GB used by self   21292: 0.096002',)\n",
      "('loading subprocess input data (model, X) took 0.01274 secs',)\n",
      "('subprocess predict took 0.61926 secs',)\n",
      "('saving subprocess output data (preds) took 0.00174 secs',)\n",
      "('[Sat Dec  2 22:09:51 2017] [predict_subprocess_xgboost] End GB used by self   21292: 0.15464',)\n",
      "('subprocess predict took 0.81171 secs',)\n",
      "('saving subprocess output data (preds) took 0.00164 secs',)\n",
      "('[Sat Dec  2 22:09:51 2017] [make_holdout_preds_subprocess] End GB used by self   21186: 0.179642',)\n",
      "('[Sat Dec  2 22:09:51 2017] [make_holdout_preds_subprocess] Begin GB used by self   21186: 0.18174',)\n",
      "('loading subprocess input data (model, X) took 0.01510 secs',)\n",
      "('[Sat Dec  2 22:09:51 2017] [predict_subprocess_xgboost] Begin GB used by self   21324: 0.0897024',)\n",
      "('loading subprocess input data (model, X) took 0.03307 secs',)\n",
      "('subprocess predict took 0.77817 secs',)\n",
      "('saving subprocess output data (preds) took 0.00181 secs',)\n",
      "('[Sat Dec  2 22:09:52 2017] [predict_subprocess_xgboost] End GB used by self   21324: 0.198324',)\n",
      "('subprocess predict took 1.00184 secs',)\n",
      "('saving subprocess output data (preds) took 0.00175 secs',)\n",
      "('[Sat Dec  2 22:09:52 2017] [make_holdout_preds_subprocess] End GB used by self   21186: 0.217305',)\n",
      "('[Sat Dec  2 22:09:52 2017] [make_holdout_preds_subprocess] Begin GB used by self   21186: 0.219402',)\n",
      "('loading subprocess input data (model, X) took 0.00737 secs',)\n",
      "('[Sat Dec  2 22:09:52 2017] [predict_subprocess_xgboost] Begin GB used by self   21356: 0.0918159',)\n",
      "('loading subprocess input data (model, X) took 0.01718 secs',)\n",
      "('subprocess predict took 0.79881 secs',)\n",
      "('saving subprocess output data (preds) took 0.00182 secs',)\n",
      "('[Sat Dec  2 22:09:53 2017] [predict_subprocess_xgboost] End GB used by self   21356: 0.172974',)\n",
      "('subprocess predict took 1.00928 secs',)\n",
      "('saving subprocess output data (preds) took 0.00269 secs',)\n",
      "('[Sat Dec  2 22:09:53 2017] [make_holdout_preds_subprocess] End GB used by self   21186: 0.219439',)\n",
      "('[Sat Dec  2 22:09:53 2017] [make_holdout_preds_subprocess] Begin GB used by self   21186: 0.221544',)\n",
      "('loading subprocess input data (model, X) took 0.01198 secs',)\n",
      "('[Sat Dec  2 22:09:53 2017] [predict_subprocess_xgboost] Begin GB used by self   21388: 0.0918077',)\n",
      "('loading subprocess input data (model, X) took 0.03318 secs',)\n",
      "('subprocess predict took 0.93940 secs',)\n",
      "('saving subprocess output data (preds) took 0.00154 secs',)\n",
      "('[Sat Dec  2 22:09:54 2017] [predict_subprocess_xgboost] End GB used by self   21388: 0.186147',)\n",
      "('subprocess predict took 1.16365 secs',)\n",
      "('saving subprocess output data (preds) took 0.00337 secs',)\n",
      "('[Sat Dec  2 22:09:54 2017] [make_holdout_preds_subprocess] End GB used by self   21186: 0.225223',)\n",
      "('[Sat Dec  2 22:09:54 2017] [make_holdout_preds_subprocess] Begin GB used by self   21186: 0.22732',)\n",
      "('loading subprocess input data (model, X) took 0.00767 secs',)\n",
      "('[Sat Dec  2 22:09:54 2017] [predict_subprocess_xgboost] Begin GB used by self   21423: 0.0918159',)\n",
      "('loading subprocess input data (model, X) took 0.02012 secs',)\n",
      "('subprocess predict took 0.67474 secs',)\n",
      "('saving subprocess output data (preds) took 0.00168 secs',)\n",
      "('[Sat Dec  2 22:09:55 2017] [predict_subprocess_xgboost] End GB used by self   21423: 0.161604',)\n",
      "('subprocess predict took 0.88735 secs',)\n",
      "('saving subprocess output data (preds) took 0.00149 secs',)\n",
      "('[Sat Dec  2 22:09:55 2017] [make_holdout_preds_subprocess] End GB used by self   21186: 0.227365',)\n",
      "('[Sat Dec  2 22:09:55 2017] [make_holdout_preds_subprocess] Begin GB used by self   21186: 0.229466',)\n",
      "('loading subprocess input data (model, X) took 0.00601 secs',)\n",
      "('[Sat Dec  2 22:09:55 2017] [predict_subprocess_xgboost] Begin GB used by self   21455: 0.0896614',)\n",
      "('loading subprocess input data (model, X) took 0.01533 secs',)\n",
      "('subprocess predict took 0.99295 secs',)\n",
      "('saving subprocess output data (preds) took 0.00179 secs',)\n",
      "('[Sat Dec  2 22:09:56 2017] [predict_subprocess_xgboost] End GB used by self   21455: 0.168403',)\n",
      "('subprocess predict took 1.19598 secs',)\n",
      "('saving subprocess output data (preds) took 0.00130 secs',)\n",
      "('[Sat Dec  2 22:09:56 2017] [make_holdout_preds_subprocess] End GB used by self   21186: 0.229495',)\n",
      "('[Sat Dec  2 22:09:56 2017] [make_holdout_preds_subprocess] Begin GB used by self   21186: 0.231592',)\n",
      "('loading subprocess input data (model, X) took 0.01442 secs',)\n",
      "('[Sat Dec  2 22:09:56 2017] [predict_subprocess_xgboost] Begin GB used by self   21487: 0.0917791',)\n",
      "('loading subprocess input data (model, X) took 0.04037 secs',)\n",
      "('subprocess predict took 1.02284 secs',)\n",
      "('saving subprocess output data (preds) took 0.00175 secs',)\n",
      "('[Sat Dec  2 22:09:58 2017] [predict_subprocess_xgboost] End GB used by self   21487: 0.211673',)\n",
      "('subprocess predict took 1.26405 secs',)\n",
      "('saving subprocess output data (preds) took 0.00153 secs',)\n",
      "('[Sat Dec  2 22:09:58 2017] [make_holdout_preds_subprocess] End GB used by self   21186: 0.23912',)\n",
      "('[Sat Dec  2 22:09:58 2017] [make_holdout_preds_subprocess] Begin GB used by self   21186: 0.241218',)\n",
      "('loading subprocess input data (model, X) took 0.00774 secs',)\n",
      "('[Sat Dec  2 22:09:58 2017] [predict_subprocess_xgboost] Begin GB used by self   21519: 0.0896901',)\n",
      "('loading subprocess input data (model, X) took 0.01682 secs',)\n",
      "('subprocess predict took 0.79913 secs',)\n",
      "('saving subprocess output data (preds) took 0.00179 secs',)\n",
      "('[Sat Dec  2 22:09:59 2017] [predict_subprocess_xgboost] End GB used by self   21519: 0.173011',)\n",
      "('subprocess predict took 0.99957 secs',)\n",
      "('saving subprocess output data (preds) took 0.00136 secs',)\n",
      "('[Sat Dec  2 22:09:59 2017] [make_holdout_preds_subprocess] End GB used by self   21186: 0.24125',)\n",
      "('[Sat Dec  2 22:09:59 2017] [make_holdout_preds_subprocess] Begin GB used by self   21186: 0.243347',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('loading subprocess input data (model, X) took 0.01058 secs',)\n",
      "('[Sat Dec  2 22:09:59 2017] [predict_subprocess_xgboost] Begin GB used by self   21551: 0.0938926',)\n",
      "('loading subprocess input data (model, X) took 0.03185 secs',)\n",
      "('subprocess predict took 0.92132 secs',)\n",
      "('saving subprocess output data (preds) took 0.00220 secs',)\n",
      "('[Sat Dec  2 22:10:00 2017] [predict_subprocess_xgboost] End GB used by self   21551: 0.179253',)\n",
      "('subprocess predict took 1.14244 secs',)\n",
      "('saving subprocess output data (preds) took 0.00146 secs',)\n",
      "('[Sat Dec  2 22:10:00 2017] [make_holdout_preds_subprocess] End GB used by self   21186: 0.243368',)\n",
      "('[Sat Dec  2 22:10:00 2017] [make_holdout_preds_subprocess] Begin GB used by self   21186: 0.243372',)\n",
      "('loading subprocess input data (model, X) took 0.00749 secs',)\n",
      "('[Sat Dec  2 22:10:00 2017] [predict_subprocess_xgboost] Begin GB used by self   21586: 0.0959734',)\n",
      "('loading subprocess input data (model, X) took 0.02244 secs',)\n",
      "('subprocess predict took 0.71325 secs',)\n",
      "('saving subprocess output data (preds) took 0.00226 secs',)\n",
      "('[Sat Dec  2 22:10:01 2017] [predict_subprocess_xgboost] End GB used by self   21586: 0.164057',)\n",
      "('subprocess predict took 0.93069 secs',)\n",
      "('saving subprocess output data (preds) took 0.00150 secs',)\n",
      "('[Sat Dec  2 22:10:01 2017] [make_holdout_preds_subprocess] End GB used by self   21186: 0.243401',)\n",
      "('[Sat Dec  2 22:10:01 2017] [make_holdout_preds_subprocess] Begin GB used by self   21186: 0.245502',)\n",
      "('loading subprocess input data (model, X) took 0.00514 secs',)\n",
      "('[Sat Dec  2 22:10:01 2017] [predict_subprocess_xgboost] Begin GB used by self   21618: 0.0959734',)\n",
      "('loading subprocess input data (model, X) took 0.01684 secs',)\n",
      "('subprocess predict took 0.76373 secs',)\n",
      "('saving subprocess output data (preds) took 0.00176 secs',)\n",
      "('[Sat Dec  2 22:10:02 2017] [predict_subprocess_xgboost] End GB used by self   21618: 0.166801',)\n",
      "('subprocess predict took 0.96965 secs',)\n",
      "('saving subprocess output data (preds) took 0.00155 secs',)\n",
      "('[Sat Dec  2 22:10:02 2017] [make_holdout_preds_subprocess] End GB used by self   21186: 0.245522',)\n",
      "('[Sat Dec  2 22:10:02 2017] [make_holdout_preds_subprocess] Begin GB used by self   21186: 0.245522',)\n",
      "('loading subprocess input data (model, X) took 0.01314 secs',)\n",
      "('[Sat Dec  2 22:10:02 2017] [predict_subprocess_xgboost] Begin GB used by self   21650: 0.0959734',)\n",
      "('loading subprocess input data (model, X) took 0.04060 secs',)\n",
      "('subprocess predict took 0.93694 secs',)\n",
      "('saving subprocess output data (preds) took 0.00169 secs',)\n",
      "('[Sat Dec  2 22:10:03 2017] [predict_subprocess_xgboost] End GB used by self   21650: 0.210764',)\n",
      "('subprocess predict took 1.18088 secs',)\n",
      "('saving subprocess output data (preds) took 0.00317 secs',)\n",
      "('[Sat Dec  2 22:10:03 2017] [make_holdout_preds_subprocess] End GB used by self   21186: 0.245535',)\n",
      "('[Sat Dec  2 22:10:03 2017] [make_holdout_preds_subprocess] Begin GB used by self   21186: 0.245539',)\n",
      "('loading subprocess input data (model, X) took 0.00661 secs',)\n",
      "('[Sat Dec  2 22:10:03 2017] [predict_subprocess_xgboost] Begin GB used by self   21682: 0.0938803',)\n",
      "('loading subprocess input data (model, X) took 0.01719 secs',)\n",
      "('subprocess predict took 0.75648 secs',)\n",
      "('saving subprocess output data (preds) took 0.00158 secs',)\n",
      "('[Sat Dec  2 22:10:04 2017] [predict_subprocess_xgboost] End GB used by self   21682: 0.172655',)\n",
      "('subprocess predict took 0.95798 secs',)\n",
      "('saving subprocess output data (preds) took 0.00149 secs',)\n",
      "('[Sat Dec  2 22:10:04 2017] [make_holdout_preds_subprocess] End GB used by self   21186: 0.245555',)\n",
      "('[Sat Dec  2 22:10:04 2017] [make_holdout_preds_subprocess] Begin GB used by self   21186: 0.247652',)\n",
      "('loading subprocess input data (model, X) took 0.01002 secs',)\n",
      "('[Sat Dec  2 22:10:04 2017] [predict_subprocess_xgboost] Begin GB used by self   21714: 0.0896901',)\n",
      "('loading subprocess input data (model, X) took 0.02936 secs',)\n",
      "('subprocess predict took 0.76162 secs',)\n",
      "('saving subprocess output data (preds) took 0.00160 secs',)\n",
      "('[Sat Dec  2 22:10:05 2017] [predict_subprocess_xgboost] End GB used by self   21714: 0.177644',)\n",
      "('subprocess predict took 0.98322 secs',)\n",
      "('saving subprocess output data (preds) took 0.00334 secs',)\n",
      "('[Sat Dec  2 22:10:05 2017] [make_holdout_preds_subprocess] End GB used by self   21186: 0.247673',)\n",
      "('[Sat Dec  2 22:10:05 2017] [make_holdout_preds_subprocess] Begin GB used by self   21186: 0.247677',)\n",
      "('loading subprocess input data (model, X) took 0.00797 secs',)\n",
      "('[Sat Dec  2 22:10:05 2017] [predict_subprocess_xgboost] Begin GB used by self   21746: 0.0917709',)\n",
      "('loading subprocess input data (model, X) took 0.02331 secs',)\n",
      "('subprocess predict took 0.78813 secs',)\n",
      "('saving subprocess output data (preds) took 0.00193 secs',)\n",
      "('[Sat Dec  2 22:10:06 2017] [predict_subprocess_xgboost] End GB used by self   21746: 0.164409',)\n",
      "('subprocess predict took 1.00297 secs',)\n",
      "('saving subprocess output data (preds) took 0.00149 secs',)\n",
      "('[Sat Dec  2 22:10:06 2017] [make_holdout_preds_subprocess] End GB used by self   21186: 0.247706',)\n",
      "('[Sat Dec  2 22:10:06 2017] [make_holdout_preds_subprocess] Begin GB used by self   21186: 0.249803',)\n",
      "('loading subprocess input data (model, X) took 0.00468 secs',)\n",
      "('[Sat Dec  2 22:10:07 2017] [predict_subprocess_xgboost] Begin GB used by self   21778: 0.091775',)\n",
      "('loading subprocess input data (model, X) took 0.01431 secs',)\n",
      "('subprocess predict took 0.75461 secs',)\n",
      "('saving subprocess output data (preds) took 0.00174 secs',)\n",
      "('[Sat Dec  2 22:10:07 2017] [predict_subprocess_xgboost] End GB used by self   21778: 0.158224',)\n",
      "('subprocess predict took 0.95418 secs',)\n",
      "('saving subprocess output data (preds) took 0.00148 secs',)\n",
      "('[Sat Dec  2 22:10:07 2017] [make_holdout_preds_subprocess] End GB used by self   21186: 0.250671',)\n",
      "('[Sat Dec  2 22:10:07 2017] [make_holdout_preds_subprocess] Begin GB used by self   21186: 0.250675',)\n",
      "('loading subprocess input data (model, X) took 0.01186 secs',)\n",
      "('[Sat Dec  2 22:10:08 2017] [predict_subprocess_xgboost] Begin GB used by self   21810: 0.0897024',)\n",
      "('loading subprocess input data (model, X) took 0.03614 secs',)\n",
      "('subprocess predict took 0.86112 secs',)\n",
      "('saving subprocess output data (preds) took 0.00182 secs',)\n",
      "('[Sat Dec  2 22:10:09 2017] [predict_subprocess_xgboost] End GB used by self   21810: 0.200839',)\n",
      "('subprocess predict took 1.09125 secs',)\n",
      "('saving subprocess output data (preds) took 0.00137 secs',)\n",
      "('[Sat Dec  2 22:10:09 2017] [make_holdout_preds_subprocess] End GB used by self   21186: 0.250696',)\n",
      "('[Sat Dec  2 22:10:09 2017] [make_holdout_preds_subprocess] Begin GB used by self   21186: 0.2507',)\n",
      "('loading subprocess input data (model, X) took 0.00562 secs',)\n",
      "('[Sat Dec  2 22:10:09 2017] [predict_subprocess_xgboost] Begin GB used by self   21842: 0.0938844',)\n",
      "('loading subprocess input data (model, X) took 0.01408 secs',)\n",
      "('subprocess predict took 0.59819 secs',)\n",
      "('saving subprocess output data (preds) took 0.00178 secs',)\n",
      "('[Sat Dec  2 22:10:09 2017] [predict_subprocess_xgboost] End GB used by self   21842: 0.166953',)\n",
      "('subprocess predict took 0.80401 secs',)\n",
      "('saving subprocess output data (preds) took 0.00142 secs',)\n",
      "('[Sat Dec  2 22:10:09 2017] [make_holdout_preds_subprocess] End GB used by self   21186: 0.250716',)\n",
      "('[Sat Dec  2 22:10:09 2017] [make_holdout_preds_subprocess] Begin GB used by self   21186: 0.25072',)\n",
      "('loading subprocess input data (model, X) took 0.00969 secs',)\n",
      "('[Sat Dec  2 22:10:10 2017] [predict_subprocess_xgboost] Begin GB used by self   21874: 0.0980746',)\n",
      "('loading subprocess input data (model, X) took 0.03052 secs',)\n",
      "('subprocess predict took 1.00322 secs',)\n",
      "('saving subprocess output data (preds) took 0.00160 secs',)\n",
      "('[Sat Dec  2 22:10:11 2017] [predict_subprocess_xgboost] End GB used by self   21874: 0.187605',)\n",
      "('subprocess predict took 1.22418 secs',)\n",
      "('saving subprocess output data (preds) took 0.00139 secs',)\n",
      "('[Sat Dec  2 22:10:11 2017] [make_holdout_preds_subprocess] End GB used by self   21186: 0.250733',)\n",
      "('[Sat Dec  2 22:10:11 2017] [make_holdout_preds_subprocess] Begin GB used by self   21186: 0.25283',)\n",
      "('loading subprocess input data (model, X) took 0.00832 secs',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[Sat Dec  2 22:10:11 2017] [predict_subprocess_xgboost] Begin GB used by self   21906: 0.0917914',)\n",
      "('loading subprocess input data (model, X) took 0.02359 secs',)\n",
      "('subprocess predict took 0.84650 secs',)\n",
      "('saving subprocess output data (preds) took 0.00178 secs',)\n",
      "('[Sat Dec  2 22:10:12 2017] [predict_subprocess_xgboost] End GB used by self   21906: 0.164852',)\n",
      "('subprocess predict took 1.05603 secs',)\n",
      "('saving subprocess output data (preds) took 0.00140 secs',)\n",
      "('[Sat Dec  2 22:10:12 2017] [make_holdout_preds_subprocess] End GB used by self   21186: 0.252846',)\n",
      "('[Sat Dec  2 22:10:12 2017] [make_holdout_preds_subprocess] Begin GB used by self   21186: 0.252854',)\n",
      "('loading subprocess input data (model, X) took 0.00430 secs',)\n",
      "('[Sat Dec  2 22:10:12 2017] [predict_subprocess_xgboost] Begin GB used by self   21938: 0.0980787',)\n",
      "('loading subprocess input data (model, X) took 0.01517 secs',)\n",
      "('subprocess predict took 0.69104 secs',)\n",
      "('saving subprocess output data (preds) took 0.00173 secs',)\n",
      "('[Sat Dec  2 22:10:13 2017] [predict_subprocess_xgboost] End GB used by self   21938: 0.157352',)\n",
      "('subprocess predict took 0.88929 secs',)\n",
      "('saving subprocess output data (preds) took 0.00129 secs',)\n",
      "('[Sat Dec  2 22:10:13 2017] [make_holdout_preds_subprocess] End GB used by self   21186: 0.252871',)\n",
      "('[Sat Dec  2 22:10:13 2017] [make_holdout_preds_subprocess] Begin GB used by self   21186: 0.252871',)\n",
      "('loading subprocess input data (model, X) took 0.01238 secs',)\n",
      "('[Sat Dec  2 22:10:13 2017] [predict_subprocess_xgboost] Begin GB used by self   21970: 0.0917832',)\n",
      "('loading subprocess input data (model, X) took 0.03941 secs',)\n",
      "('subprocess predict took 0.92094 secs',)\n",
      "('saving subprocess output data (preds) took 0.00159 secs',)\n",
      "('[Sat Dec  2 22:10:14 2017] [predict_subprocess_xgboost] End GB used by self   21970: 0.210653',)\n",
      "('subprocess predict took 1.15964 secs',)\n",
      "('saving subprocess output data (preds) took 0.00136 secs',)\n",
      "('[Sat Dec  2 22:10:14 2017] [make_holdout_preds_subprocess] End GB used by self   21186: 0.252883',)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.87132628,  0.9488554 ,  0.93376813, ...,  0.94583185,\n",
       "        0.79710248,  0.93223693])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score the train data\n",
    "scorer.score_batch(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_v1</th>\n",
       "      <th>1_v10</th>\n",
       "      <th>2_v101</th>\n",
       "      <th>3_v102</th>\n",
       "      <th>4_v103</th>\n",
       "      <th>5_v104</th>\n",
       "      <th>6_v105</th>\n",
       "      <th>7_v106</th>\n",
       "      <th>8_v109</th>\n",
       "      <th>9_v11</th>\n",
       "      <th>...</th>\n",
       "      <th>119_CV_CatNumEnc_v110_v66__v1_median</th>\n",
       "      <th>119_CV_CatNumEnc_v110_v66__v11_median</th>\n",
       "      <th>119_CV_CatNumEnc_v110_v66__v4_median</th>\n",
       "      <th>119_CV_CatNumEnc_v110_v66__v40_median</th>\n",
       "      <th>119_CV_CatNumEnc_v110_v66__v5_median</th>\n",
       "      <th>119_CV_CatNumEnc_v110_v66__v50_median</th>\n",
       "      <th>119_CV_CatNumEnc_v110_v66__v6_median</th>\n",
       "      <th>120_NumCatTE_v1_v12_v5_v50_v56_v6_v66_v7_v79_0</th>\n",
       "      <th>121_WoE_v113_v22_v30_v72_v75_0</th>\n",
       "      <th>122_WoE_v113_v22_v56_v66_v75_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.335739</td>\n",
       "      <td>0.503281</td>\n",
       "      <td>8.389237</td>\n",
       "      <td>2.757375</td>\n",
       "      <td>4.374296</td>\n",
       "      <td>1.574039</td>\n",
       "      <td>0.007294</td>\n",
       "      <td>12.579184</td>\n",
       "      <td>3.930922e+00</td>\n",
       "      <td>16.434108</td>\n",
       "      <td>...</td>\n",
       "      <td>1.444466</td>\n",
       "      <td>15.446568</td>\n",
       "      <td>4.406222</td>\n",
       "      <td>8.568845</td>\n",
       "      <td>8.906318</td>\n",
       "      <td>1.418824</td>\n",
       "      <td>2.445583</td>\n",
       "      <td>0.761595</td>\n",
       "      <td>1.161431</td>\n",
       "      <td>1.161431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.797415</td>\n",
       "      <td>6.542669</td>\n",
       "      <td>8.507281</td>\n",
       "      <td>2.503055</td>\n",
       "      <td>4.872157</td>\n",
       "      <td>2.573664</td>\n",
       "      <td>0.113967</td>\n",
       "      <td>12.554274</td>\n",
       "      <td>1.990131e+00</td>\n",
       "      <td>16.347483</td>\n",
       "      <td>...</td>\n",
       "      <td>1.458727</td>\n",
       "      <td>15.507245</td>\n",
       "      <td>4.272285</td>\n",
       "      <td>10.815426</td>\n",
       "      <td>8.713364</td>\n",
       "      <td>1.324921</td>\n",
       "      <td>2.437831</td>\n",
       "      <td>0.761595</td>\n",
       "      <td>1.161431</td>\n",
       "      <td>1.161431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.050328</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.442494</td>\n",
       "      <td>15.492702</td>\n",
       "      <td>4.254459</td>\n",
       "      <td>5.836770</td>\n",
       "      <td>8.446665</td>\n",
       "      <td>1.188468</td>\n",
       "      <td>2.367107</td>\n",
       "      <td>0.761595</td>\n",
       "      <td>1.161431</td>\n",
       "      <td>1.161431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.312910</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.435345</td>\n",
       "      <td>15.507402</td>\n",
       "      <td>4.151858</td>\n",
       "      <td>10.256855</td>\n",
       "      <td>8.466711</td>\n",
       "      <td>1.025603</td>\n",
       "      <td>2.377665</td>\n",
       "      <td>0.777084</td>\n",
       "      <td>1.161431</td>\n",
       "      <td>1.161431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.344477</td>\n",
       "      <td>1.050329</td>\n",
       "      <td>7.940586</td>\n",
       "      <td>2.329044</td>\n",
       "      <td>4.751472</td>\n",
       "      <td>2.281442</td>\n",
       "      <td>0.075693</td>\n",
       "      <td>12.360166</td>\n",
       "      <td>-2.394541e-07</td>\n",
       "      <td>16.003086</td>\n",
       "      <td>...</td>\n",
       "      <td>1.442494</td>\n",
       "      <td>15.492702</td>\n",
       "      <td>4.254459</td>\n",
       "      <td>5.836770</td>\n",
       "      <td>8.446665</td>\n",
       "      <td>1.188468</td>\n",
       "      <td>2.367107</td>\n",
       "      <td>0.761595</td>\n",
       "      <td>1.161431</td>\n",
       "      <td>1.161431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  147 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0_v1     1_v10    2_v101    3_v102    4_v103    5_v104    6_v105  \\\n",
       "0  1.335739  0.503281  8.389237  2.757375  4.374296  1.574039  0.007294   \n",
       "1  0.797415  6.542669  8.507281  2.503055  4.872157  2.573664  0.113967   \n",
       "2       NaN  1.050328       NaN       NaN       NaN       NaN       NaN   \n",
       "3       NaN  1.312910       NaN       NaN       NaN       NaN       NaN   \n",
       "4  1.344477  1.050329  7.940586  2.329044  4.751472  2.281442  0.075693   \n",
       "\n",
       "      7_v106        8_v109      9_v11               ...                \\\n",
       "0  12.579184  3.930922e+00  16.434108               ...                 \n",
       "1  12.554274  1.990131e+00  16.347483               ...                 \n",
       "2        NaN           NaN        NaN               ...                 \n",
       "3        NaN           NaN        NaN               ...                 \n",
       "4  12.360166 -2.394541e-07  16.003086               ...                 \n",
       "\n",
       "   119_CV_CatNumEnc_v110_v66__v1_median  \\\n",
       "0                              1.444466   \n",
       "1                              1.458727   \n",
       "2                              1.442494   \n",
       "3                              1.435345   \n",
       "4                              1.442494   \n",
       "\n",
       "   119_CV_CatNumEnc_v110_v66__v11_median  \\\n",
       "0                              15.446568   \n",
       "1                              15.507245   \n",
       "2                              15.492702   \n",
       "3                              15.507402   \n",
       "4                              15.492702   \n",
       "\n",
       "   119_CV_CatNumEnc_v110_v66__v4_median  \\\n",
       "0                              4.406222   \n",
       "1                              4.272285   \n",
       "2                              4.254459   \n",
       "3                              4.151858   \n",
       "4                              4.254459   \n",
       "\n",
       "   119_CV_CatNumEnc_v110_v66__v40_median  \\\n",
       "0                               8.568845   \n",
       "1                              10.815426   \n",
       "2                               5.836770   \n",
       "3                              10.256855   \n",
       "4                               5.836770   \n",
       "\n",
       "   119_CV_CatNumEnc_v110_v66__v5_median  \\\n",
       "0                              8.906318   \n",
       "1                              8.713364   \n",
       "2                              8.446665   \n",
       "3                              8.466711   \n",
       "4                              8.446665   \n",
       "\n",
       "   119_CV_CatNumEnc_v110_v66__v50_median  \\\n",
       "0                               1.418824   \n",
       "1                               1.324921   \n",
       "2                               1.188468   \n",
       "3                               1.025603   \n",
       "4                               1.188468   \n",
       "\n",
       "   119_CV_CatNumEnc_v110_v66__v6_median  \\\n",
       "0                              2.445583   \n",
       "1                              2.437831   \n",
       "2                              2.367107   \n",
       "3                              2.377665   \n",
       "4                              2.367107   \n",
       "\n",
       "   120_NumCatTE_v1_v12_v5_v50_v56_v6_v66_v7_v79_0  \\\n",
       "0                                        0.761595   \n",
       "1                                        0.761595   \n",
       "2                                        0.761595   \n",
       "3                                        0.777084   \n",
       "4                                        0.761595   \n",
       "\n",
       "   121_WoE_v113_v22_v30_v72_v75_0  122_WoE_v113_v22_v56_v66_v75_0  \n",
       "0                        1.161431                        1.161431  \n",
       "1                        1.161431                        1.161431  \n",
       "2                        1.161431                        1.161431  \n",
       "3                        1.161431                        1.161431  \n",
       "4                        1.161431                        1.161431  \n",
       "\n",
       "[5 rows x 147 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform test data\n",
    "transformed_test = scorer.transform_batch(test_data)\n",
    "transformed_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train H2O Model \n",
    "\n",
    "We are not limited to using the scores from Driverless AI.  We can also use the transformed data to train our own model.  In this section, we will train a GBM model on the raw data and transformed data and compare the results to Driverless AI. \n",
    "\n",
    "#### 1. Train H2O Model on Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h2o==3.16.0.1 from http://h2o-release.s3.amazonaws.com/h2o/rel-wheeler/1/Python/h2o-3.16.0.1-py2.py3-none-any.whl in /h2oai_env/lib/python3.6/site-packages\r\n",
      "Requirement already satisfied: colorama>=0.3.8 in /h2oai_env/lib/python3.6/site-packages (from h2o==3.16.0.1)\r\n",
      "Requirement already satisfied: requests in /h2oai_env/lib/python3.6/site-packages (from h2o==3.16.0.1)\r\n",
      "Requirement already satisfied: tabulate in /h2oai_env/lib/python3.6/site-packages (from h2o==3.16.0.1)\r\n",
      "Requirement already satisfied: future in /h2oai_env/lib/python3.6/site-packages (from h2o==3.16.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "# Install H2O\n",
    "! pip install http://h2o-release.s3.amazonaws.com/h2o/rel-wheeler/1/Python/h2o-3.16.0.1-py2.py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54323..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: java version \"1.8.0_151\"; Java(TM) SE Runtime Environment (build 1.8.0_151-b12); Java HotSpot(TM) 64-Bit Server VM (build 25.151-b12, mixed mode)\n",
      "  Starting server from /h2oai_env/lib/python3.6/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmpuc0yr3ej\n",
      "  JVM stdout: /tmp/tmpuc0yr3ej/h2o_unknownUser_started_from_python.out\n",
      "  JVM stderr: /tmp/tmpuc0yr3ej/h2o_unknownUser_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54323\n",
      "Connecting to H2O server at http://127.0.0.1:54323... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>01 secs</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.16.0.1</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>8 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_unknownUser_of43v7</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>13.33 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54323</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.2 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ----------------------------------------\n",
       "H2O cluster uptime:         01 secs\n",
       "H2O cluster version:        3.16.0.1\n",
       "H2O cluster version age:    8 days\n",
       "H2O cluster name:           H2O_from_python_unknownUser_of43v7\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    13.33 Gb\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  4\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54323\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.2 final\n",
       "--------------------------  ----------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import H2O and connect\n",
    "import h2o\n",
    "h2o.init(port = 54323)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: || 100%\n",
      "Parse progress: || 100%\n"
     ]
    }
   ],
   "source": [
    "train_h2o = h2o.import_file(train_path, col_types = {target: \"enum\"})\n",
    "test_h2o = h2o.import_file(test_path, col_types = {target: \"enum\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm Model Build progress: || 100%\n"
     ]
    }
   ],
   "source": [
    "from h2o.estimators import H2OGradientBoostingEstimator\n",
    "gbm_raw = H2OGradientBoostingEstimator(model_id = \"raw_data\", \n",
    "                                       nfolds = 3, # 3-fold cross validation\n",
    "                                       ntrees = 1000,   # more than enough trees\n",
    "                                       ## early stopping once the validation AUC doesn't improve \n",
    "                                       ## by at least 0.1% for 3 consecutive scoring events\n",
    "                                       stopping_rounds = 3, stopping_tolerance = 1e-3, stopping_metric = \"AUC\", \n",
    "                                       score_tree_interval = 10,\n",
    "                                       seed = 1234)\n",
    "gbm_raw.train(y = target, training_frame = train_h2o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Train H2O Model on Transformed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateTransformedH2OFrame(path, scorer, target):\n",
    "    \n",
    "    # Import Data\n",
    "    data = pd.read_csv(path)\n",
    "    \n",
    "    # Transform Data with scoring package\n",
    "    transformed_data = scorer.transform_batch(data)\n",
    "    transformed_data = pd.concat([transformed_data, data[target]], axis = 1)\n",
    "    \n",
    "    # Convert to H2O Frame\n",
    "    transformed_h2o = h2o.H2OFrame(transformed_data, column_types = {target:\"enum\"})\n",
    "    \n",
    "    return transformed_h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: || 100%\n",
      "Parse progress: || 100%\n"
     ]
    }
   ],
   "source": [
    "transformed_train_h2o = CreateTransformedH2OFrame(train_path, scorer, target)\n",
    "transformed_test_h2o = CreateTransformedH2OFrame(test_path, scorer, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm Model Build progress: || 100%\n"
     ]
    }
   ],
   "source": [
    "gbm_transformed = H2OGradientBoostingEstimator(model_id = \"transformed_data\", \n",
    "                                               nfolds = 3, # 3-fold cross validation\n",
    "                                               ntrees = 1000,   # more than enough trees \n",
    "                                               ## early stopping once the validation AUC doesn't improve \n",
    "                                               ## by at least 0.1% for 3 consecutive scoring events\n",
    "                                               stopping_rounds = 3, stopping_tolerance = 1e-3, stopping_metric = \"AUC\",\n",
    "                                               score_tree_interval = 10,\n",
    "                                               seed = 1234)\n",
    "gbm_transformed.train(y = target, training_frame = transformed_train_h2o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_auc = gbm_raw.model_performance(test_data = test_h2o).auc()\n",
    "transformed_data_auc = gbm_transformed.model_performance(test_data = transformed_test_h2o).auc()\n",
    "driverless_auc = experiment.test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Data AUC: 0.7533\n",
      "Transformed Data AUC: 0.6182\n",
      "Driverless AI AUC: 0.7822\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw Data AUC: \" + str(round(raw_data_auc, 4)))\n",
    "print(\"Transformed Data AUC: \" + str(round(transformed_data_auc, 4)))\n",
    "print(\"Driverless AI AUC: \" + str(round(driverless_auc, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
