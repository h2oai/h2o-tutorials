{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Good Amazon Reviews\n",
    "\n",
    "For this demo, we will be using the [Amazon Fine Food Reviews Data](https://www.kaggle.com/snap/amazon-fine-food-reviews).  The Amazon Fine Food Reviews dataset consists of 568,454 food reviews Amazon users left up to October 2012.\n",
    "\n",
    "This script is based off of the [Craigslist Word2Vec Demo](https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/word2vec_craigslistjobtitles.ipynb).\n",
    "\n",
    "Our Machine Learning Workflow is: \n",
    "\n",
    "1. Import data into H2O\n",
    "2. Exploratory Analysis\n",
    "3. Tokenize Text\n",
    "4. Train Word2Vec Model\n",
    "5. Analyze Word Embeddings\n",
    "6. Train Positive Review Model with Original Data\n",
    "7. Train Positive Review Model with Word Embeddings\n",
    "8. Run AutoML\n",
    "9. View AutoML in Flow\n",
    "10. Shutdown H2O cluster\n",
    "\n",
    "# Step 1 (of 10).  Import data into H2O\n",
    "\n",
    "We will begin by importing our review data into our H2O cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.init(max_mem_size=\"8G\", bind_to_localhost=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into H2O\n",
    "\n",
    "# https://s3-us-west-2.amazonaws.com/h2o-tutorials/data/topics/nlp/amazon_reviews/AmazonReviews.csv\n",
    "reviews = h2o.import_file(\"../../data/topics/nlp/amazon_reviews/AmazonReviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 (of 10).  Exploratory Analysis\n",
    "\n",
    "We will start our analysis by exploring the dataset and creating our target column.  In this case, we would like to predict whether or not a reviewer liked the product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reviews.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"Score\"].table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_freq = reviews[\"Summary\"].table()\n",
    "summary_freq = summary_freq.sort(\"Count\", ascending = [False])\n",
    "summary_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#' Add Target Column: \"PositiveReview\"\n",
    "reviews[\"PositiveReview\"] = (reviews[\"Score\"] >= 4).ifelse(\"1\", \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"PositiveReview\"].table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 (of 10).  Tokenize Words\n",
    "\n",
    "\n",
    "We will tokenize the words in the review column. We will do this by creating a function called `tokenize`.  This will split the reviews into words and remove any stop words, small words, or words with numbers in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Stop Words\n",
    "# The STOP WORDS we are importing are from the nltk package\n",
    "import pandas as pd\n",
    "\n",
    "# https://s3-us-west-2.amazonaws.com/h2o-tutorials/data/topics/nlp/amazon_reviews/stopwords.csv\n",
    "STOP_WORDS = pd.read_csv(\"../../data/topics/nlp/amazon_reviews/stopwords.csv\", header=0)\n",
    "STOP_WORDS = list(STOP_WORDS['STOP_WORD'])\n",
    "\n",
    "STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences, stop_word = STOP_WORDS):\n",
    "    tokenized = sentences.tokenize(\"\\\\W+\")\n",
    "    tokenized_lower = tokenized.tolower()\n",
    "    tokenized_filtered = tokenized_lower[(tokenized_lower.nchar() >= 2) | (tokenized_lower.isna()),:]\n",
    "    tokenized_words = tokenized_filtered[tokenized_filtered.grep(\"[0-9]\",invert=True,output_logical=True),:]\n",
    "    tokenized_words = tokenized_words[(tokenized_words.isna()) | (~ tokenized_words.isin(STOP_WORDS)),:]\n",
    "    return tokenized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break reviews into sequence of words\n",
    "words = tokenize(reviews[\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 (of 10).  Train Word2Vec Model\n",
    "\n",
    "Now that we've tokenized our words, we can train a word2vec model. We will start by creating word embeddings of length 2.  We will use these to understand the word embedding since they can be easily visualized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec Model for vec size = 2\n",
    "from h2o.estimators.word2vec import H2OWord2vecEstimator\n",
    "\n",
    "w2v_len2_model = H2OWord2vecEstimator(vec_size = 2, model_id = \"w2v_len2.hex\")\n",
    "w2v_len2_model.train(training_frame=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 (of 10).  Analyze Word Embeddings\n",
    "\n",
    "We have created word embeddings for each word in our corpus, now we want to understand what they mean and how to interpret them.  The word embeddings for the first few words are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_embeddings = words.head()\n",
    "sample_embeddings.col_names = [\"Word\"]\n",
    "sample_embeddings = sample_embeddings.cbind(w2v_len2_model.transform(sample_embeddings, aggregate_method=\"None\"))\n",
    "\n",
    "sample_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will plot the word embeddings to see which words are related to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = w2v_len2_model.to_frame()\n",
    "word_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Word Embeddings to selected words\n",
    "\n",
    "selected_words = [\"coffee\", \"espresso\", \"starbucks\", \"sweet\", \"salty\", \"savory\", \"email\", \"support\", \"answered\", \n",
    "                  \"unhappy\", \"waited\", \"returned\", \"tasty\", \"yummy\", \"moldy\", \"expired\", \"salmonella\", \"best\", \n",
    "                  \"amazing\", \"abdominal\", \"folic\", \"zinc\"]\n",
    "\n",
    "filtered_embeddings = word_embeddings[word_embeddings[\"Word\"].isin(selected_words)]\n",
    "plot_data = filtered_embeddings.as_data_frame(use_pandas = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Word Embeddings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "ax.scatter(plot_data[\"V1\"], plot_data[\"V2\"])\n",
    "\n",
    "for i, txt in enumerate(plot_data[\"Word\"]):\n",
    "    ax.annotate(txt, (plot_data[\"V1\"].iloc[i], plot_data[\"V2\"].iloc[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We will train a more complex word2vec model with length 100 - this will be able to catch further nuances in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec Model for vec size = 100\n",
    "w2v_model = H2OWord2vecEstimator(vec_size = 100, model_id = \"w2v.hex\")\n",
    "w2v_model.train(training_frame=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to interpret the word embeddings is to use them to find synonyms. The `find_synonyms` function finds the words that have the smallest cosine distance in word embeddings.  We assume that if the word embeddings are similar, the two words are synonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check - find synonyms for the word 'coffee'\n",
    "w2v_model.find_synonyms(\"coffee\", count = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check - find synonyms for the word 'stale'\n",
    "w2v_model.find_synonyms(\"stale\", count = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will cluster the word embeddings to identify segments of similar words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = w2v_model.to_frame()\n",
    "word_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators import H2OKMeansEstimator\n",
    "\n",
    "kmeans = H2OKMeansEstimator(model_id = \"word_segments.hex\",\n",
    "                            estimate_k = True, k = 100, # Max number of clusters\n",
    "                            seed = 1234)\n",
    "\n",
    "x = list(set(word_embeddings.col_names) - set([\"Word\"]))\n",
    "kmeans.train(x = x, training_frame = word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clusters\n",
    "kmeans.centroid_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_clusters = word_embeddings.cbind(kmeans.predict(word_embeddings))\n",
    "word_clusters[[\"Word\", \"predict\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_clusters = word_clusters[word_clusters[\"Word\"].isin(selected_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cluster 0\")\n",
    "print(selected_clusters[selected_clusters[\"predict\"] == 0, \"Word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cluster 1\")\n",
    "print(selected_clusters[selected_clusters[\"predict\"] == 1, \"Word\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 (of 10).  Train Model with Original Data\n",
    "\n",
    "Now that we have analyzed our word embeddings, we will turn to our supervised learning task to predict whether someone liked the review.  We will first train a model on our original data - not our word embeddings to see what our accuracy is. We will use this model as a baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 80th quantile of time in the dataset\n",
    "time_split = reviews[\"Time\"].quantile(prob = [0.8])[1]\n",
    "reviews[\"Train\"] = (reviews[\"Time\"] < time_split).ifelse(\"Yes\", \"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = reviews[reviews[\"Train\"] == \"Yes\"]\n",
    "test = reviews[reviews[\"Train\"] == \"No\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators import H2OGradientBoostingEstimator\n",
    "\n",
    "predictors = ['ProductId', 'UserId', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Time']\n",
    "response = 'PositiveReview'\n",
    "\n",
    "gbm_baseline = H2OGradientBoostingEstimator(stopping_metric = \"AUC\", stopping_tolerance = 0.001,\n",
    "                                            stopping_rounds = 5, score_tree_interval = 10,\n",
    "                                            model_id = \"gbm_baseline.hex\"\n",
    "                                           )\n",
    "gbm_baseline.train(x = predictors, y = response, \n",
    "                   training_frame = train, validation_frame = test\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC on Validation Data: \" + str(round(gbm_baseline.auc(valid = True), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a big room for improvement.  Our error is 22%.  To improve our model, we will train word embeddings for the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_baseline.confusion_matrix(valid = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable importance plot below shows us that the most important variable is `HelpfulnessNumerator`.  Looking at the partial dependency plot for that variable, we see that the more people who find the review helpful, the more likely it is a good review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_baseline.varimp_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "pdp_helpfulness = gbm_baseline.partial_plot(train, cols = [\"HelpfulnessNumerator\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7 (of 10).  Train Model with Word Embeddings\n",
    "\n",
    "We will train a GBM model with the same parameters as our baseline gbm.  This time, however, we will add the aggregated word embeddings as predictors. Then we will use the features the GBM model identified as important to train a GLM model to see if we can get a similar performance from a simpler linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a vector for each review\n",
    "review_vecs = w2v_model.transform(words, aggregate_method = \"AVERAGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "review_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add aggregated word embeddings \n",
    "ext_reviews = reviews.cbind(review_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_train = ext_reviews[ext_reviews[\"Train\"] == \"Yes\"]\n",
    "ext_test = ext_reviews[ext_reviews[\"Train\"] == \"No\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = predictors + review_vecs.names\n",
    "response = 'PositiveReview'\n",
    "\n",
    "gbm_embeddings = H2OGradientBoostingEstimator(stopping_metric = \"AUC\", stopping_tolerance = 0.001,\n",
    "                                              stopping_rounds = 5, score_tree_interval = 10,\n",
    "                                              model_id = \"gbm_embeddings.hex\", ntrees = 1000,\n",
    "                                             )\n",
    "gbm_embeddings.train(x = predictors, y = response, \n",
    "                   training_frame = ext_train, validation_frame = ext_test\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline AUC: \" + str(round(gbm_baseline.auc(valid = True), 3)))\n",
    "print(\"With Embeddings AUC: \" + str(round(gbm_embeddings.auc(valid = True), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_embeddings.confusion_matrix(valid = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_embeddings.varimp_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a simpler GLM model using important word2vec features of the GBM model to generate interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_w2v_features = list(filter(lambda x: x.startswith('C'), map(lambda x: x[0], gbm_embeddings.varimp())))[1:10]\n",
    "top_w2v_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators import H2OGeneralizedLinearEstimator\n",
    "\n",
    "glm_predictors = [\"HelpfulnessNumerator\", \"HelpfulnessDenominator\"] + review_vecs.names\n",
    "\n",
    "glm_embeddings = H2OGeneralizedLinearEstimator(model_id = \"glm_embeddings.hex\", interactions = top_w2v_features,\n",
    "                                               family = \"binomial\"\n",
    "                                              )\n",
    "glm_embeddings.train(x = glm_predictors, y = response,\n",
    "                     training_frame = ext_train, validation_frame = ext_test\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline AUC: \" + str(round(gbm_baseline.auc(valid = True), 3)))\n",
    "print(\"With Embeddings AUC (GBM): \" + str(round(gbm_embeddings.auc(valid = True), 3)))\n",
    "print(\"With Embeddings AUC (GLM): \" + str(round(glm_embeddings.auc(valid = True), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_embeddings.confusion_matrix(valid = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Step 8 (of 10).  Run AutoML\n",
    "\n",
    "We will use H2O's AutoML to see if we can improve the performance further by exploring the algorithm and parameter space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "automl = H2OAutoML(project_name = \"positive_reviews\", max_runtime_secs = 180, \n",
    "                   keep_cross_validation_models = False, keep_cross_validation_predictions = False,\n",
    "                   nfolds = 3, exclude_algos = [\"DRF\"], seed = 1234)\n",
    "automl.train(x = predictors, y = response, training_frame = ext_train, leaderboard_frame = ext_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl.leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9 (of 10). Watch AutoML progress (in the H2O Flow Web UI)Â¶\n",
    "\n",
    "* Go to port 54321\n",
    "* In H2O Flow, go to Admin -> Jobs\n",
    "* Click on the \"Auto Model\" job with the \"positive_reviews\" job name and explore it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Step 10 (of 10).  Shutdown the H2O Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.cluster().shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Github location for this tutorial\n",
    "\n",
    "* https://github.com/h2oai/h2o-tutorials/tree/master/training/h2o_3_hands_on/nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
